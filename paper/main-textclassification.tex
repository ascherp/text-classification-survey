\pdfoutput=1
%\documentclass[lettersize,journal]{IEEEtran}
%\documentclass[onecolumn,12pt]{IEEEtran}
\documentclass[acmsmall,nonacm]{acmart}

% FOR ARXIV
%\documentclass[manuscript,nonacm]{acmart}

% FOR Submission
%\documentclass[manuscript,review]{acmart}

% Final
% \documentclass[acmsmall]{acmart}

\usepackage{amsmath,amsfonts}

\acmJournal{CSUR}

\usepackage{dsbda-style}
\usepackage{hyperref}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{threeparttable}
\usepackage{longtable}

\newcommand{\mlp}{WideMLP\xspace}

\newcommand{\mytextsubscript}[1]{{\color{black}\textsubscript{#1}}}
\newcommand{\myheader}{}

\newcommand{\GenLM}{generative language model\xspace} 
\newcommand{\GenLMs}{generative language models\xspace}

\newcommand{\SLM}{SLM\xspace}
\newcommand{\SLMs}{SLMs\xspace}
\newcommand{\LLM}{LLM\xspace}
\newcommand{\LLMs}{LLMs\xspace}

\let\mycite\cite
\newcommand\myflag[0]{our experiment}
%\let\mytablefontsize\small
\let\mytablefontsize\footnotesize

% \usepackage{cite}

\begin{document}

\title{Are We Really Making Much Progress in Text Classification? A Comparative Review}

%\author{Lukas Galke, Ansgar Scherp, Andor Diera, Bao Xin Lin, \\
%Bhakti Khera, Tim Meuser, Tushar~Singhal, and Fabian~Karl
%\thanks{L. Galke is with the University of Southern Denmark}
%\thanks{A. Scherp, A. Diera, B. Lin, B. Khera, T. Meuser, and F. Karl are with the University of Ulm, Germany}
%}

\author{Lukas Galke}
\orcid{0000-0001-6124-1092}
\affiliation{\institution{University of Southern Denmark}
\city{Odense}
\country{Denmark}}
\email{galke@imada.sdu.dk}

\author{Ansgar Scherp}
\orcid{0000-0002-2653-9245}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{ansgar.scherp@uni-ulm.de}

\author{Andor Diera}
\orcid{0009-0001-3959-493X}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{andor.diera@uni-ulm.de}

\author{Fabian Karl}
\orcid{0009-0008-0079-5604}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{fabian.karl@uni-ulm.de}

\author{Bao Xin Lin}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{bao.lin@uni-ulm.de}

\author{Bhakti Khera}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{bhakti.khera@uni-ulm.de}

\author{Tim Meuser}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{tim.meuser@uni-ulm.de}

\author{Tushar Singhal}
\affiliation{\institution{University of Ulm} \country{Germany}}
\email{tushar.singhal@uni-ulm.de}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
       <concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002944.10011122.10002945</concept_id>
       <concept_desc>General and reference~Surveys and overviews</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003347.10003356<concept_id>
       <concept_desc>Information systems~Clustering and classification</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Supervised learning by classification}
\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{General and reference~Surveys and overviews}
\ccsdesc[300]{Information systems~Clustering and classification}

\renewcommand{\shortauthors}{Galke et al.}

\begin{abstract}

We analyze various methods for single-label and multi-label text classification across well-known datasets, categorizing them into bag-of-words, sequence-based, graph-based, and hierarchical approaches. 
Despite the surge in methods like graph-based models, encoder-only pre-trained language models, notably BERT, remain state-of-the-art. 
However, recent findings suggest simpler models like logistic regression and trigram-based SVMs outperform newer techniques. 
While decoder-only generative language models show promise in learning with limited data, they lag behind encoder-only models in performance. 
We emphasize the superiority of discriminative language models like BERT over generative models for supervised tasks. 
Additionally, we highlight the literature's lack of robustness in method comparisons, particularly concerning basic hyperparameter optimizations like learning rate in fine-tuning encoder-only language models.

\noindent \textbf{Data availability}: The source code is available at \url{https://github.com/drndr/multilabel-text-clf}. All datasets used for our experiments are publicly available except the NYT dataset.

\end{abstract}

\maketitle
\clearpage
\tableofcontents
\clearpage

%\begin{IEEEkeywords}
%Data mining, text mining, text classification, neural networks, machine learning
%\end{IEEEkeywords}

\input{content}

%\bibliographystyle{IEEEtran}
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
